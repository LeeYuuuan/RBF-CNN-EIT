{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1PRA-qnqtfXUIyc8zFtMLqhPXdY52oPM9",
      "authorship_tag": "ABX9TyN6oIhprqAPp76YkTvQWQQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeYuuuan/RBF-CNN-EIT/blob/main/RBF_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m3cOrqn6b27o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset,DataLoader,Dataset\n",
        "from torch.autograd import Variable # 获取变量\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(r\"/content/drive/MyDrive/Colab Notebooks/EIT/EITDeepLearning\")\n",
        "import torch_rbf as rbf"
      ],
      "metadata": {
        "id": "8jvGlzJUeTgo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xzewsjFbbQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DealDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, a):\n",
        "        \n",
        "        # load the .csv simulation data\n",
        "        \n",
        "        dic_path = \"/content/drive/MyDrive/Colab Notebooks/EIT/EITDeepLearning/data/\"\n",
        "        X = np.loadtxt(dic_path + 'datacsv1.csv', delimiter=';', dtype=np.float32)\n",
        "        y = np.loadtxt(dic_path + 'datacsv.csv', delimiter=';', dtype=np.float32)\n",
        "\n",
        "        # split train/test dataset\n",
        "        X_train1, X_test1, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "        X_train = np.zeros([X_train1.shape[0], 1, 16, 12]) # shape: count * 1 * 16(angle) * 12(voltage)\n",
        "        X_test = np.zeros([X_test1.shape[0], 1, 16, 12])\n",
        "\n",
        "        # return different data\n",
        "        if a == 'train':\n",
        "            self.x = X_train\n",
        "            self.y = y_train\n",
        "            self.len = X_train.shape[0]\n",
        "        elif a == 'test':\n",
        "            self.x = X_test\n",
        "            self.y = y_test\n",
        "            self.len = X_test.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "EdZJsZdjfWFC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义网络结构\n",
        "class CNNnet(torch.nn.Module):\n",
        "    def __init__(self, layer_widths, layer_centres, basis_func):\n",
        "        super(CNNnet,self).__init__()\n",
        "        self.rbf_layers = nn.ModuleList()\n",
        "        self.linear_layers = nn.ModuleList()\n",
        "        self.conv1_1 = nn.Sequential(\n",
        "            # (1, 16, 12)pytorch输入不用定义大小，因为参数过多写成nn.Conv2d(1,64,3,1，1)形式容易出错，有的参数默认了\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, # 单通道\n",
        "                out_channels=64, # 64通道\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1 # 零填充\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(64), # 批量归一化\n",
        "            nn.ELU(), # ELU激活函数\n",
        "        )\n",
        "        self.conv1_2 = nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2) # 最大池化层，核大小为2\n",
        "        )\n",
        "        self.conv2_1 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv2_2 = nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=128,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(128),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3_1 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=128,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv3_2 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv3_3 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv3_4 = nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=256,\n",
        "                out_channels=256,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4_1 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=256,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv4_2 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv4_3 = nn.Sequential(\n",
        "            # (1, 28, 28)\n",
        "            nn.Conv2d(\n",
        "                in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "        self.conv4_4 = nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=512,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,  # 卷积filter, 移动块长\n",
        "                stride=1,  # filter的每次移动步长\n",
        "                padding=1\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(512),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(kernel_size=2,ceil_mode=True) # 平均池化层，核大小为2，2×1经过这层变成1×0，报错，但有ceil_mode=True为向上取整，变成1×1\n",
        "        )\n",
        "        for i in range(len(layer_widths) - 1):\n",
        "            self.rbf_layers.append(rbf.RBF(layer_widths[i], layer_centres[i], basis_func))\n",
        "            self.linear_layers.append(nn.Linear(layer_centres[i], layer_widths[i + 1]))\n",
        "        self.mlp2 = torch.nn.Linear(512,576) # 输出层，512是输入这层的大小，576是输出的大小\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward  propagation\"\"\"\n",
        "        x = self.conv1_1(x)\n",
        "        x = self.conv1_2(x)\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.conv3_2(x)\n",
        "        x = self.conv3_3(x)\n",
        "        x = self.conv3_4(x)\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.conv4_3(x)\n",
        "        x = self.conv4_4(x)\n",
        "        x = x.view(x.size(0),-1) # 四维数据（数据量，通道，纵向，横向），展开为二维（数据量，只有横向）\n",
        "        for i in range(len(self.rbf_layers)):\n",
        "            x = self.rbf_layers[i](x)\n",
        "            x = self.linear_layers[i](x)\n",
        "        x = self.mlp2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yATezHZWmQcF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Dataset\n",
        "dealDataset1 = DealDataset(a = 'train')\n",
        "\n",
        "#将这个类传给DataLoader\n",
        "train_loader = DataLoader(dataset=dealDataset1,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True)"
      ],
      "metadata": {
        "id": "BL014BErl65q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_widths = [512] #神经元数量\n",
        "layer_centres = [40]\n",
        "basis_func = rbf.gaussian #初始偏置\n",
        "\n",
        "model = CNNnet(layer_widths, layer_centres, basis_func) #初始化模型\n",
        "print(model)  #显示模型参数\n",
        "model = model.float() #模型参数转为float型\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device) #训练数据和模型数据类型要统一，并且pytorch-gpu数据类型要变成指定类型\n",
        "\n",
        "loss_func = torch.nn.MSELoss() #定义损失函数\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.3, momentum=0.9) #动量梯度下降法\n",
        "\n",
        "dealDataset2 = DealDataset(a='test')  #测试集数据\n",
        "\n",
        "test_loader = DataLoader(dataset=dealDataset2,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True)\n"
      ],
      "metadata": {
        "id": "rtjJPbU4ngK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36962d43-8dec-4744-8c4f-73a342f40a20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNnet(\n",
            "  (rbf_layers): ModuleList()\n",
            "  (linear_layers): ModuleList()\n",
            "  (conv1_1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv1_2): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2_1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv2_2): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3_1): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv3_2): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv3_3): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv3_4): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4_1): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv4_2): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv4_3): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "  )\n",
            "  (conv4_4): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ELU(alpha=1.0)\n",
            "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  )\n",
            "  (mlp2): Linear(in_features=512, out_features=576, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "下面矩阵存放训练过程中损失函数等数据\n",
        "'''\n",
        "loss_count = []\n",
        "iter_loss = []\n",
        "batch_loss = []\n",
        "test_loss = []\n",
        "epochs=100 "
      ],
      "metadata": {
        "id": "zMq-xL1LnswX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    for i, (x, y)in enumerate(train_loader):\n",
        "        batch_x = Variable(x).to(torch.float32) # torch.Size([, 1, 16, 12])\n",
        "        batch_x = batch_x.cuda()\n",
        "        batch_y = Variable(y).to(torch.float32) # torch.Size([576])\n",
        "        batch_y = batch_y.cuda()\n",
        "        out = model(batch_x)        # 获取最后输出\n",
        "        loss = loss_func(out, batch_y)        # 获取损失\n",
        "        batch_loss.append(loss.cuda().data.cpu().numpy())    # 使用优化器优化损失\n",
        "        opt.zero_grad()  # 清空上一步残余更新参数值\n",
        "        loss.backward() # 误差反向传播，计算参数更新值\n",
        "        opt.step() # 将参数更新值施加到net的parmeters上\n",
        "        '''\n",
        "        每20个数据显示一次训练集均方误差\n",
        "        '''\n",
        "        if i%20 == 0:\n",
        "            loss_count.append(loss)\n",
        "            print('train MSE {}:\\t'.format(i), loss.item())\n",
        "            torch.save(model,r'C:\\Users\\luoxinyu\\Desktop\\log_CNN')\n",
        "\n",
        "    for a,b in test_loader:\n",
        "        test_x = Variable(a).to(torch.float32)\n",
        "        test_x = test_x.cuda()\n",
        "        test_y = Variable(b).to(torch.float32)\n",
        "        test_y = test_y.cuda()\n",
        "        out = model(test_x)\n",
        "        loss = loss_func(out, test_y)\n",
        "        test_loss.append(loss.cuda().data.cpu().numpy())\n",
        "        '''\n",
        "        每轮示一次训练集均方误差\n",
        "        '''\n",
        "        print('test MSE {}:\\t'.format(epoch+1), loss.item())\n",
        "        break\n",
        "    iter_loss.append(np.average(np.array(batch_loss)))\n",
        "\n",
        "'''\n",
        "绘制测试集 轮数与均方误差图像\n",
        "'''\n",
        "x = np.arange(epochs)\n",
        "y = np.array(iter_loss)\n",
        "plt.plot(x, y)\n",
        "plt.title('pic')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D3C_5M-BmH0o",
        "outputId": "b50fb705-54ef-4308-d7aa-870cb2b39cb5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train MSE 0:\t 0.03799193352460861\n",
            "train MSE 20:\t 0.004057553596794605\n",
            "train MSE 40:\t 0.001961037516593933\n",
            "test MSE 1:\t 0.0020100853871554136\n",
            "train MSE 0:\t 0.0020636555273085833\n",
            "train MSE 20:\t 0.0020505045540630817\n",
            "train MSE 40:\t 0.001980851637199521\n",
            "test MSE 2:\t 0.0019653327763080597\n",
            "train MSE 0:\t 0.001971866236999631\n",
            "train MSE 20:\t 0.0019671262707561255\n",
            "train MSE 40:\t 0.001994298305362463\n",
            "test MSE 3:\t 0.0019770741928368807\n",
            "train MSE 0:\t 0.0019717891700565815\n",
            "train MSE 20:\t 0.0019195672357454896\n",
            "train MSE 40:\t 0.0019442939665168524\n",
            "test MSE 4:\t 0.001961636357009411\n",
            "train MSE 0:\t 0.0019878887105733156\n",
            "train MSE 20:\t 0.0020274207927286625\n",
            "train MSE 40:\t 0.0018743477994576097\n",
            "test MSE 5:\t 0.0019282115390524268\n",
            "train MSE 0:\t 0.001978071639314294\n",
            "train MSE 20:\t 0.001945390016771853\n",
            "train MSE 40:\t 0.0019526465330272913\n",
            "test MSE 6:\t 0.0019341792212799191\n",
            "train MSE 0:\t 0.0019337489502504468\n",
            "train MSE 20:\t 0.001955359010025859\n",
            "train MSE 40:\t 0.0019993220921605825\n",
            "test MSE 7:\t 0.0019361716695129871\n",
            "train MSE 0:\t 0.001936452928930521\n",
            "train MSE 20:\t 0.001999468542635441\n",
            "train MSE 40:\t 0.0019620107486844063\n",
            "test MSE 8:\t 0.0019448869861662388\n",
            "train MSE 0:\t 0.001952307065948844\n",
            "train MSE 20:\t 0.0019563513342291117\n",
            "train MSE 40:\t 0.0019292222568765283\n",
            "test MSE 9:\t 0.0019730785861611366\n",
            "train MSE 0:\t 0.0019393517868593335\n",
            "train MSE 20:\t 0.001953741768375039\n",
            "train MSE 40:\t 0.001982262823730707\n",
            "test MSE 10:\t 0.0018936623819172382\n",
            "train MSE 0:\t 0.0019907988607883453\n",
            "train MSE 20:\t 0.0020038015209138393\n",
            "train MSE 40:\t 0.0019087514374405146\n",
            "test MSE 11:\t 0.001979327294975519\n",
            "train MSE 0:\t 0.0019522263901308179\n",
            "train MSE 20:\t 0.0019480428891256452\n",
            "train MSE 40:\t 0.0019433633424341679\n",
            "test MSE 12:\t 0.00200141710229218\n",
            "train MSE 0:\t 0.00196835701353848\n",
            "train MSE 20:\t 0.0019688184838742018\n",
            "train MSE 40:\t 0.001994917169213295\n",
            "test MSE 13:\t 0.0019886235240846872\n",
            "train MSE 0:\t 0.0019795626867562532\n",
            "train MSE 20:\t 0.0019639991223812103\n",
            "train MSE 40:\t 0.0018335474887862802\n",
            "test MSE 14:\t 0.0019729421474039555\n",
            "train MSE 0:\t 0.001996449427679181\n",
            "train MSE 20:\t 0.0019263984868302941\n",
            "train MSE 40:\t 0.002003380795940757\n",
            "test MSE 15:\t 0.001972250174731016\n",
            "train MSE 0:\t 0.001973968930542469\n",
            "train MSE 20:\t 0.0019661434926092625\n",
            "train MSE 40:\t 0.001983122667297721\n",
            "test MSE 16:\t 0.0019247563323006034\n",
            "train MSE 0:\t 0.0019407852087169886\n",
            "train MSE 20:\t 0.0019385280320420861\n",
            "train MSE 40:\t 0.001973559847101569\n",
            "test MSE 17:\t 0.0018999645253643394\n",
            "train MSE 0:\t 0.001887747785076499\n",
            "train MSE 20:\t 0.0019623099360615015\n",
            "train MSE 40:\t 0.0019802236929535866\n",
            "test MSE 18:\t 0.001962018199265003\n",
            "train MSE 0:\t 0.001892748405225575\n",
            "train MSE 20:\t 0.001966790994629264\n",
            "train MSE 40:\t 0.0019482746720314026\n",
            "test MSE 19:\t 0.0019277078099548817\n",
            "train MSE 0:\t 0.0019064719090238214\n",
            "train MSE 20:\t 0.001982195535674691\n",
            "train MSE 40:\t 0.0019550372380763292\n",
            "test MSE 20:\t 0.0019476668676361442\n",
            "train MSE 0:\t 0.0019920894410461187\n",
            "train MSE 20:\t 0.0019882144406437874\n",
            "train MSE 40:\t 0.001958493608981371\n",
            "test MSE 21:\t 0.0020090206526219845\n",
            "train MSE 0:\t 0.0018828068859875202\n",
            "train MSE 20:\t 0.0018652474973350763\n",
            "train MSE 40:\t 0.0019703612197190523\n",
            "test MSE 22:\t 0.0019281740533187985\n",
            "train MSE 0:\t 0.001935396809130907\n",
            "train MSE 20:\t 0.001940978690981865\n",
            "train MSE 40:\t 0.0019660622347146273\n",
            "test MSE 23:\t 0.001998977502807975\n",
            "train MSE 0:\t 0.001975198509171605\n",
            "train MSE 20:\t 0.002014017663896084\n",
            "train MSE 40:\t 0.00196828693151474\n",
            "test MSE 24:\t 0.0019905520603060722\n",
            "train MSE 0:\t 0.0018527422798797488\n",
            "train MSE 20:\t 0.0019774618558585644\n",
            "train MSE 40:\t 0.0018464819295331836\n",
            "test MSE 25:\t 0.001991692464798689\n",
            "train MSE 0:\t 0.001985731301829219\n",
            "train MSE 20:\t 0.001962489914149046\n",
            "train MSE 40:\t 0.00194651132915169\n",
            "test MSE 26:\t 0.0019456318113952875\n",
            "train MSE 0:\t 0.002004193374887109\n",
            "train MSE 20:\t 0.0019789759535342455\n",
            "train MSE 40:\t 0.0019301734864711761\n",
            "test MSE 27:\t 0.001897167065180838\n",
            "train MSE 0:\t 0.0019435880240052938\n",
            "train MSE 20:\t 0.001981565961614251\n",
            "train MSE 40:\t 0.001967874588444829\n",
            "test MSE 28:\t 0.0019300931598991156\n",
            "train MSE 0:\t 0.0019056482706218958\n",
            "train MSE 20:\t 0.0019525624811649323\n",
            "train MSE 40:\t 0.001990051008760929\n",
            "test MSE 29:\t 0.0020175608806312084\n",
            "train MSE 0:\t 0.0018458225531503558\n",
            "train MSE 20:\t 0.0019387278007343411\n",
            "train MSE 40:\t 0.0019327841000631452\n",
            "test MSE 30:\t 0.0019547438714653254\n",
            "train MSE 0:\t 0.0020013973116874695\n",
            "train MSE 20:\t 0.0019115611212328076\n",
            "train MSE 40:\t 0.0019819866865873337\n",
            "test MSE 31:\t 0.002005988033488393\n",
            "train MSE 0:\t 0.0018738984363153577\n",
            "train MSE 20:\t 0.0019367741188034415\n",
            "train MSE 40:\t 0.001992773963138461\n",
            "test MSE 32:\t 0.0019678883254528046\n",
            "train MSE 0:\t 0.001954291947185993\n",
            "train MSE 20:\t 0.0019219852983951569\n",
            "train MSE 40:\t 0.0019380615558475256\n",
            "test MSE 33:\t 0.001955876126885414\n",
            "train MSE 0:\t 0.00194734544493258\n",
            "train MSE 20:\t 0.0019744618330150843\n",
            "train MSE 40:\t 0.0019637520890682936\n",
            "test MSE 34:\t 0.0020039607770740986\n",
            "train MSE 0:\t 0.0019619138911366463\n",
            "train MSE 20:\t 0.001990074524655938\n",
            "train MSE 40:\t 0.0019284476293250918\n",
            "test MSE 35:\t 0.0019549792632460594\n",
            "train MSE 0:\t 0.0019313970115035772\n",
            "train MSE 20:\t 0.0019588980358093977\n",
            "train MSE 40:\t 0.001883664052002132\n",
            "test MSE 36:\t 0.0019234694773331285\n",
            "train MSE 0:\t 0.0019196780631318688\n",
            "train MSE 20:\t 0.0020085887517780066\n",
            "train MSE 40:\t 0.002009207382798195\n",
            "test MSE 37:\t 0.0019022648921236396\n",
            "train MSE 0:\t 0.002000992652028799\n",
            "train MSE 20:\t 0.0020212174858897924\n",
            "train MSE 40:\t 0.0019475426524877548\n",
            "test MSE 38:\t 0.0018524854676797986\n",
            "train MSE 0:\t 0.0019280515844002366\n",
            "train MSE 20:\t 0.0018981066532433033\n",
            "train MSE 40:\t 0.0019960536155849695\n",
            "test MSE 39:\t 0.0019068906549364328\n",
            "train MSE 0:\t 0.0019027928356081247\n",
            "train MSE 20:\t 0.001989309908822179\n",
            "train MSE 40:\t 0.0020134011283516884\n",
            "test MSE 40:\t 0.0019610198214650154\n",
            "train MSE 0:\t 0.001890991348773241\n",
            "train MSE 20:\t 0.0019938398618251085\n",
            "train MSE 40:\t 0.0018645260715857148\n",
            "test MSE 41:\t 0.0019236882217228413\n",
            "train MSE 0:\t 0.0018575605936348438\n",
            "train MSE 20:\t 0.0019400041783228517\n",
            "train MSE 40:\t 0.0019403869519010186\n",
            "test MSE 42:\t 0.0019472328713163733\n",
            "train MSE 0:\t 0.0019146245904266834\n",
            "train MSE 20:\t 0.0019535396713763475\n",
            "train MSE 40:\t 0.00200494471937418\n",
            "test MSE 43:\t 0.0019985325634479523\n",
            "train MSE 0:\t 0.0019295811653137207\n",
            "train MSE 20:\t 0.0019939292687922716\n",
            "train MSE 40:\t 0.0019129953579977155\n",
            "test MSE 44:\t 0.0019400459714233875\n",
            "train MSE 0:\t 0.0020127175375819206\n",
            "train MSE 20:\t 0.0019291784847155213\n",
            "train MSE 40:\t 0.0019842134788632393\n",
            "test MSE 45:\t 0.0019129605498164892\n",
            "train MSE 0:\t 0.0018972770776599646\n",
            "train MSE 20:\t 0.0018860368290916085\n",
            "train MSE 40:\t 0.002028117422014475\n",
            "test MSE 46:\t 0.0019983535166829824\n",
            "train MSE 0:\t 0.002013342222198844\n",
            "train MSE 20:\t 0.0019413443515077233\n",
            "train MSE 40:\t 0.0019724939484149218\n",
            "test MSE 47:\t 0.0019658179953694344\n",
            "train MSE 0:\t 0.0020236631389707327\n",
            "train MSE 20:\t 0.0019307293696328998\n",
            "train MSE 40:\t 0.001940294518135488\n",
            "test MSE 48:\t 0.001990760676562786\n",
            "train MSE 0:\t 0.0020253395196050406\n",
            "train MSE 20:\t 0.0020216964185237885\n",
            "train MSE 40:\t 0.001976841129362583\n",
            "test MSE 49:\t 0.001959575340151787\n",
            "train MSE 0:\t 0.0019091876456514\n",
            "train MSE 20:\t 0.0019447588128969073\n",
            "train MSE 40:\t 0.0019398948643356562\n",
            "test MSE 50:\t 0.0019427136285230517\n",
            "train MSE 0:\t 0.0018267405685037374\n",
            "train MSE 20:\t 0.001968561438843608\n",
            "train MSE 40:\t 0.0020026080310344696\n",
            "test MSE 51:\t 0.0019124679965898395\n",
            "train MSE 0:\t 0.0019127425039187074\n",
            "train MSE 20:\t 0.0019455511355772614\n",
            "train MSE 40:\t 0.001893037697300315\n",
            "test MSE 52:\t 0.0019434724235907197\n",
            "train MSE 0:\t 0.0019760739523917437\n",
            "train MSE 20:\t 0.001994080375880003\n",
            "train MSE 40:\t 0.001930013531818986\n",
            "test MSE 53:\t 0.001925373449921608\n",
            "train MSE 0:\t 0.0019526223186403513\n",
            "train MSE 20:\t 0.001977649750187993\n",
            "train MSE 40:\t 0.001936664222739637\n",
            "test MSE 54:\t 0.001955494051799178\n",
            "train MSE 0:\t 0.0019295135280117393\n",
            "train MSE 20:\t 0.0019306524191051722\n",
            "train MSE 40:\t 0.0019209185848012567\n",
            "test MSE 55:\t 0.0019255183869972825\n",
            "train MSE 0:\t 0.0019538940396159887\n",
            "train MSE 20:\t 0.001851870329119265\n",
            "train MSE 40:\t 0.0019317960832268\n",
            "test MSE 56:\t 0.0020120092667639256\n",
            "train MSE 0:\t 0.001971294404938817\n",
            "train MSE 20:\t 0.0020426956471055746\n",
            "train MSE 40:\t 0.0019489191472530365\n",
            "test MSE 57:\t 0.0019399337470531464\n",
            "train MSE 0:\t 0.002032194286584854\n",
            "train MSE 20:\t 0.001942283590324223\n",
            "train MSE 40:\t 0.0019613925833255053\n",
            "test MSE 58:\t 0.0019396187271922827\n",
            "train MSE 0:\t 0.001904828124679625\n",
            "train MSE 20:\t 0.001974672544747591\n",
            "train MSE 40:\t 0.0019969982095062733\n",
            "test MSE 59:\t 0.0019356533885002136\n",
            "train MSE 0:\t 0.001924429670907557\n",
            "train MSE 20:\t 0.0019278994295746088\n",
            "train MSE 40:\t 0.0019523708615452051\n",
            "test MSE 60:\t 0.001982457470148802\n",
            "train MSE 0:\t 0.001892019878141582\n",
            "train MSE 20:\t 0.0019721619319170713\n",
            "train MSE 40:\t 0.0019605623092502356\n",
            "test MSE 61:\t 0.0019177314825356007\n",
            "train MSE 0:\t 0.001932922750711441\n",
            "train MSE 20:\t 0.002016128273680806\n",
            "train MSE 40:\t 0.0019953057635575533\n",
            "test MSE 62:\t 0.0019586628768593073\n",
            "train MSE 0:\t 0.001893688808195293\n",
            "train MSE 20:\t 0.0019943166989833117\n",
            "train MSE 40:\t 0.001989961601793766\n",
            "test MSE 63:\t 0.0018823490245267749\n",
            "train MSE 0:\t 0.0019925949163734913\n",
            "train MSE 20:\t 0.0019788010977208614\n",
            "train MSE 40:\t 0.0019738285336643457\n",
            "test MSE 64:\t 0.001973776612430811\n",
            "train MSE 0:\t 0.0019299983978271484\n",
            "train MSE 20:\t 0.0019227766897529364\n",
            "train MSE 40:\t 0.0019926412496715784\n",
            "test MSE 65:\t 0.001968253403902054\n",
            "train MSE 0:\t 0.001908911974169314\n",
            "train MSE 20:\t 0.00191546231508255\n",
            "train MSE 40:\t 0.002013640245422721\n",
            "test MSE 66:\t 0.0019726434256881475\n",
            "train MSE 0:\t 0.001971554011106491\n",
            "train MSE 20:\t 0.001904972712509334\n",
            "train MSE 40:\t 0.0018919432768598199\n",
            "test MSE 67:\t 0.0019197004148736596\n",
            "train MSE 0:\t 0.001965165603905916\n",
            "train MSE 20:\t 0.0018732030875980854\n",
            "train MSE 40:\t 0.00195826543495059\n",
            "test MSE 68:\t 0.0019416076829656959\n",
            "train MSE 0:\t 0.0019231707556173205\n",
            "train MSE 20:\t 0.001864296500571072\n",
            "train MSE 40:\t 0.0019301556749269366\n",
            "test MSE 69:\t 0.0019871736876666546\n",
            "train MSE 0:\t 0.0019393848488107324\n",
            "train MSE 20:\t 0.0019156999187543988\n",
            "train MSE 40:\t 0.0019359249854460359\n",
            "test MSE 70:\t 0.0019499299814924598\n",
            "train MSE 0:\t 0.0018894515233114362\n",
            "train MSE 20:\t 0.0018753771437332034\n",
            "train MSE 40:\t 0.00188407429959625\n",
            "test MSE 71:\t 0.001957088243216276\n",
            "train MSE 0:\t 0.0018392223864793777\n",
            "train MSE 20:\t 0.0018649643752723932\n",
            "train MSE 40:\t 0.001932129729539156\n",
            "test MSE 72:\t 0.0019413584377616644\n",
            "train MSE 0:\t 0.0018804126884788275\n",
            "train MSE 20:\t 0.0019322900334373116\n",
            "train MSE 40:\t 0.0019645392894744873\n",
            "test MSE 73:\t 0.001966762123629451\n",
            "train MSE 0:\t 0.002001747954636812\n",
            "train MSE 20:\t 0.0018697112100198865\n",
            "train MSE 40:\t 0.0019226885633543134\n",
            "test MSE 74:\t 0.0020124095026403666\n",
            "train MSE 0:\t 0.0019577937200665474\n",
            "train MSE 20:\t 0.0019126521656289697\n",
            "train MSE 40:\t 0.0019012575503438711\n",
            "test MSE 75:\t 0.0019661809783428907\n",
            "train MSE 0:\t 0.001941715250723064\n",
            "train MSE 20:\t 0.0019254423677921295\n",
            "train MSE 40:\t 0.001971785444766283\n",
            "test MSE 76:\t 0.0019210973987355828\n",
            "train MSE 0:\t 0.001964044524356723\n",
            "train MSE 20:\t 0.0020342154894024134\n",
            "train MSE 40:\t 0.0019696045201271772\n",
            "test MSE 77:\t 0.0019348148489370942\n",
            "train MSE 0:\t 0.001991608878597617\n",
            "train MSE 20:\t 0.001993251731619239\n",
            "train MSE 40:\t 0.00192292092833668\n",
            "test MSE 78:\t 0.001977761974558234\n",
            "train MSE 0:\t 0.0019290639320388436\n",
            "train MSE 20:\t 0.0019388362998142838\n",
            "train MSE 40:\t 0.00184996472671628\n",
            "test MSE 79:\t 0.001924522453919053\n",
            "train MSE 0:\t 0.0018930714577436447\n",
            "train MSE 20:\t 0.0020231183152645826\n",
            "train MSE 40:\t 0.0019064383814111352\n",
            "test MSE 80:\t 0.0020208070054650307\n",
            "train MSE 0:\t 0.00195537437684834\n",
            "train MSE 20:\t 0.0019806590862572193\n",
            "train MSE 40:\t 0.001945218537002802\n",
            "test MSE 81:\t 0.0019729912746697664\n",
            "train MSE 0:\t 0.0019955697935074568\n",
            "train MSE 20:\t 0.0020120975095778704\n",
            "train MSE 40:\t 0.0019037696765735745\n",
            "test MSE 82:\t 0.0019475953886285424\n",
            "train MSE 0:\t 0.0018667487893253565\n",
            "train MSE 20:\t 0.0019144803518429399\n",
            "train MSE 40:\t 0.0019569953437894583\n",
            "test MSE 83:\t 0.0019347729394212365\n",
            "train MSE 0:\t 0.0019458601018413901\n",
            "train MSE 20:\t 0.0018911095103248954\n",
            "train MSE 40:\t 0.0019180361414328218\n",
            "test MSE 84:\t 0.002000330248847604\n",
            "train MSE 0:\t 0.001954291947185993\n",
            "train MSE 20:\t 0.0019006989896297455\n",
            "train MSE 40:\t 0.0020017256028950214\n",
            "test MSE 85:\t 0.0019407827639952302\n",
            "train MSE 0:\t 0.001989176031202078\n",
            "train MSE 20:\t 0.0018991207471117377\n",
            "train MSE 40:\t 0.0019507916877046227\n",
            "test MSE 86:\t 0.0019385156920179725\n",
            "train MSE 0:\t 0.0019448244711384177\n",
            "train MSE 20:\t 0.0019859352614730597\n",
            "train MSE 40:\t 0.0019150120206177235\n",
            "test MSE 87:\t 0.0019885029178112745\n",
            "train MSE 0:\t 0.0018854513764381409\n",
            "train MSE 20:\t 0.0019260924309492111\n",
            "train MSE 40:\t 0.002037550788372755\n",
            "test MSE 88:\t 0.0020077016670256853\n",
            "train MSE 0:\t 0.0019721905700862408\n",
            "train MSE 20:\t 0.002016552025452256\n",
            "train MSE 40:\t 0.0019955127499997616\n",
            "test MSE 89:\t 0.0020215746480971575\n",
            "train MSE 0:\t 0.001965619856491685\n",
            "train MSE 20:\t 0.0019239240791648626\n",
            "train MSE 40:\t 0.00196429924108088\n",
            "test MSE 90:\t 0.0019933904986828566\n",
            "train MSE 0:\t 0.0019273824291303754\n",
            "train MSE 20:\t 0.002011682838201523\n",
            "train MSE 40:\t 0.0018910521175712347\n",
            "test MSE 91:\t 0.001996475039049983\n",
            "train MSE 0:\t 0.0019315456738695502\n",
            "train MSE 20:\t 0.0019206301076337695\n",
            "train MSE 40:\t 0.002002634573727846\n",
            "test MSE 92:\t 0.0019422279438003898\n",
            "train MSE 0:\t 0.0019456115551292896\n",
            "train MSE 20:\t 0.0019048653775826097\n",
            "train MSE 40:\t 0.0020439045038074255\n",
            "test MSE 93:\t 0.0019389933440834284\n",
            "train MSE 0:\t 0.001899665454402566\n",
            "train MSE 20:\t 0.0019483193755149841\n",
            "train MSE 40:\t 0.001913993270136416\n",
            "test MSE 94:\t 0.0019158412469550967\n",
            "train MSE 0:\t 0.002035801997408271\n",
            "train MSE 20:\t 0.0019430669490247965\n",
            "train MSE 40:\t 0.0019660615362226963\n",
            "test MSE 95:\t 0.0019599786028265953\n",
            "train MSE 0:\t 0.001987026073038578\n",
            "train MSE 20:\t 0.00192600570153445\n",
            "train MSE 40:\t 0.0020381563808768988\n",
            "test MSE 96:\t 0.0019112055888399482\n",
            "train MSE 0:\t 0.0019642156548798084\n",
            "train MSE 20:\t 0.0018782487604767084\n",
            "train MSE 40:\t 0.0020256973803043365\n",
            "test MSE 97:\t 0.0019987549167126417\n",
            "train MSE 0:\t 0.0019096711184829473\n",
            "train MSE 20:\t 0.0019160807132720947\n",
            "train MSE 40:\t 0.00184631475713104\n",
            "test MSE 98:\t 0.001957387663424015\n",
            "train MSE 0:\t 0.0019361211452633142\n",
            "train MSE 20:\t 0.0019667460583150387\n",
            "train MSE 40:\t 0.0019409205997362733\n",
            "test MSE 99:\t 0.0020311586558818817\n",
            "train MSE 0:\t 0.0019161251839250326\n",
            "train MSE 20:\t 0.0017907180590555072\n",
            "train MSE 40:\t 0.0018871093634516\n",
            "test MSE 100:\t 0.001993864309042692\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2MWuF_kbl6jw"
      }
    }
  ]
}